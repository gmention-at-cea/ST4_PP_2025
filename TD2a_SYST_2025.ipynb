{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6a8104e6-f318-4b91-a7d7-11d83cc36dfd",
      "metadata": {
        "id": "6a8104e6-f318-4b91-a7d7-11d83cc36dfd"
      },
      "source": [
        "\n",
        "# <font color=\"darkred\" size=\"+4\">**Particle Physics**</font>\n",
        "Centrale-Supélec - ST4 - 2024\n",
        "\n",
        "<font color=\"darkred\" size=\"+3\">**Work on systematics**</font>\n",
        "\n",
        "<br/>\n",
        "\n",
        "---\n",
        "## <font color=\"darkred\">**General Instructions**</font>\n",
        "---\n",
        "<br/>\n",
        "\n",
        "To do this homework, you have to download it on your own \"Google Colaboratory\" space (see in the \"File\" menu, \"save a copy in your Google Drive\", you will have to create a gmail account if you don't already have one) so that you can modify it as you wish. You need to train with Google Colaboratory as you will be using it for the Challenging Week (Enseignement d'Intégration).\n",
        "\n",
        "Once you did indeed save this notebook on your own Google drive, you can modify it and save it again. If you corrupt anything by mistake, you have automatic recording of previous version which you can still acess to get back. Anyway you can still start from scratch with the default notebook if you experience really weird things with your current version.\n",
        "\n",
        "On the Help menu above, (menu \"Aide\" in French) you can switch the language of the interface. For this lectures I set it to English.\n",
        "\n",
        "Last notice: in this notebook we will use greek characters in the code cells. To be able to write yourself these greek characters you need to install greek keyboard on your laptop, or to copy paste the ones provided in the code cells.\n",
        "\n",
        "\n",
        "## <font color=\"darkred\">**Overview**</font>\n",
        "\n",
        "> The purpose of this practical exercise is to apply the statistical and systematic methods covered in Lectures 3 and 4.\n",
        ">\n",
        "> We will address the following topics:\n",
        ">\n",
        ">\n",
        "> 1.   Simple Plots computations of basic statistical and systematic uncertainties\n",
        "> 2.   Propagating systematic uncertainties through a pre-trained BDT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28453367-d44d-4f94-96f9-46b92f2d9258",
      "metadata": {
        "id": "28453367-d44d-4f94-96f9-46b92f2d9258"
      },
      "source": [
        "## <font color=\"darkred\">**Signal and backgounds**</font>\n",
        "\n",
        "When we collect measurements from sensors over a short period, we term them as events. Events vary in structure, ranging from sensor values to complex derived quantities. Often, we cannot definitively identify if we have detected what we sought in the recorded information.\n",
        "\n",
        "We categorize events broadly into two types:\n",
        "\n",
        "- Signal: desired detection.\n",
        "- Background: unwanted data collected alongside.\n",
        "\n",
        "The name of the game is to check if observed proportions of signal and backgrounds are in line with our predictions.\n",
        "\n",
        "## <font color=\"darkred\">**Prediction framework**</font>\n",
        "\n",
        "Imagine we have a dataset containing both signal and background events, and we only know the total event count, $n_{\\rm obs}$. We have formulated a model to predict the numbers of signal ($S$) and background ($B$) events. Our aim is to verify if our predicted counts match the actual counts in the dataset, particularly focusing on the signal category.\n",
        "\n",
        "To achieve this, we employ a parametric model\n",
        "\n",
        "$n_{\\rm pred}({\\boldsymbol\\mu})={\\boldsymbol\\mu}S+B$\n",
        "\n",
        "where S and B are fixed, but $\\boldsymbol\\mu$, denoted as the signal strength, is what we seek to determine.\n",
        "- If our hypothesis, $n_{\\rm pred}=\\mu S+B$, aligns with the data, $n_{\\rm obs}=S+B$, we expect the total recorded events to match our model's prediction, from which we derive the signal strength being $\\mu=1$.\n",
        "- If the observed number of events is from background only, $n_{\\rm obs}=B$ then we get $\\mu=0$.\n",
        "- Other possibilities arise because of statistical fluctuations (observation process being random by nature) or systematic errors in the determination of $S$ and/or $B$.\n",
        "\n",
        "Statistical fluctuations occur because of the random nature of the measurement process at stake.\n",
        "\n",
        "Systematical errors come into play because of incomplete knowledge of the experimental conditions and/or theoretical prediction framework.\n",
        "\n",
        "The verification process about possible $\\mu$ values is what we will undertake next."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b25491d-d2a9-41a5-b7a6-1cad3c8d1865",
      "metadata": {
        "id": "3b25491d-d2a9-41a5-b7a6-1cad3c8d1865"
      },
      "source": [
        "## <font color=\"darkred\">**Statistical modeling**</font>\n",
        "\n",
        "As we have seen in the lecture, $n_{\\rm obs}$ should follow a Poisson distribution:\n",
        "\n",
        "$${\\cal P}(n|S, B) = \\frac{\\left(S+B\\right)^{n_{\\rm obs}}}{n_{\\rm obs}!}e^{-\\left(\\,S+B\\,\\right)}$$\n",
        "\n",
        "In particular we have the first two moments:\n",
        "- ${\\mathbb E}[n_{\\rm obs}]= S+B$\n",
        "- ${\\mathbb V}[n_{\\rm obs}]= S+B$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5935c83d",
      "metadata": {
        "id": "5935c83d"
      },
      "source": [
        "We start by first importing prerequisited packages and modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1816449b",
      "metadata": {
        "metadata": {},
        "id": "1816449b"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c01ea3e",
      "metadata": {
        "id": "2c01ea3e"
      },
      "source": [
        "\n",
        "# <font color=\"darkred\">**Part 1: On statistical behaviours of uncertainties**</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c8fab50-2645-457b-8f1a-dfc1ecd15fe2",
      "metadata": {
        "id": "8c8fab50-2645-457b-8f1a-dfc1ecd15fe2"
      },
      "source": [
        "## **<font color=\"darkred\">QUESTION 1</font>**\n",
        "\n",
        "Match the observed number of events, $n_{\\rm obs}$ to the expected number of events from the model\n",
        "\n",
        "$$n_{\\rm exp}=\\mu S+B$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the value of $\\mu$ as a function of $n_{\\rm obs}$, $S$ and $B$"
      ],
      "metadata": {
        "id": "Xzw3SN99wNMY"
      },
      "id": "Xzw3SN99wNMY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4d6bea2-b305-4fc4-b785-3ce41ca24daa",
      "metadata": {
        "metadata": {},
        "id": "f4d6bea2-b305-4fc4-b785-3ce41ca24daa"
      },
      "outputs": [],
      "source": [
        "### Type your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "669bbdac-4cc3-4917-b5e7-79ce0d677cb1",
      "metadata": {
        "id": "669bbdac-4cc3-4917-b5e7-79ce0d677cb1"
      },
      "source": [
        "Without data, in the experiment design phase, we often resort to Monte Carlo simulations by sampling synthetic data around expectations to get realistic simulated datasets.\n",
        "\n",
        "Any deviation in $n_{\\rm obs}$ with respect to $S+B$ will result in deviation in $\\mu$ with respect to 1.\n",
        "\n",
        "Therefore because of randomness of $n_{\\rm obs}$ around mean value ${\\mathbb E}[n_{\\rm obs}]=S+B$ following the Poisson distribution, $\\mu$ will be random around 1."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "730fb6bf-e8ad-4469-b405-d7303134bc73",
      "metadata": {
        "id": "730fb6bf-e8ad-4469-b405-d7303134bc73"
      },
      "source": [
        "## **<font color=\"darkred\">QUESTION 2</font>**\n",
        "\n",
        "What are:\n",
        "1. the expectation, ${\\mathbb E}[\\mu]$,\n",
        "2. the variance ${\\mathbb V}[\\mu]$,\n",
        "3. the standard deviation $\\sigma_{\\mu}$?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bot6HBU0K6km"
      },
      "id": "bot6HBU0K6km"
    },
    {
      "cell_type": "markdown",
      "id": "8d278bb3-3e8b-4f91-ad57-22f3d0bf7fbb",
      "metadata": {
        "id": "8d278bb3-3e8b-4f91-ad57-22f3d0bf7fbb"
      },
      "source": [
        "To start with (not to bother with random sampling) we first assume that we collect the expected number of event $n_{\\rm obs}={\\mathbb E}[n]=S+B$ (_i.e._ without any statistical fluctuation). This peculiar situation is called the \"*Asimov dataset*\"<a name=\"cite_ref-1\"></a>[<sup>[^1]</sup>](#cite_note-1). <a name=\"back_to_citation_1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3facf2f1-1b71-4663-852d-01d7ccf463cf",
      "metadata": {
        "id": "3facf2f1-1b71-4663-852d-01d7ccf463cf"
      },
      "source": [
        "To study the evolution of $\\sigma_\\mu=\\sqrt{{\\mathbb V}[\\mu]}$ with respect to $n_{\\rm obs}$ with will consider the experiment is taking data over an increasing amount of time. We will simply comsider the $n_{obs}$, $S$ and $B$ will be proportional to some factor $\\alpha$.\n",
        "\n",
        "Take for this application:\n",
        "1. $S=100$\n",
        "2. $B=10,000$\n",
        "3. $\\alpha$ from 1 to 1,000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb1f97df",
      "metadata": {
        "id": "bb1f97df"
      },
      "source": [
        "Write down a code to compute sigma_mu as a function of alpha values for S and B fixed and plot sigma_mu as function of alpha.\n",
        "1. First in Linear scale (using `np.linspace` for alpha and plotting with `plt.plot`)\n",
        "2. Second in loglog scale (using `np.logspace` for alpha and plotting with\n",
        " `plt.loglog`)\n",
        "\n",
        " Do not forget to name axes with `plt.xlabel(...)` and `plt.ylabel(...)` when producing graphics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc381a20",
      "metadata": {
        "metadata": {},
        "id": "bc381a20"
      },
      "outputs": [],
      "source": [
        "α = ...\n",
        "\n",
        "def sigma_mu(α, S=100, B=10_000):\n",
        "    return ...\n",
        "\n",
        "# plt.plot(...)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d3179b7",
      "metadata": {
        "id": "7d3179b7"
      },
      "source": [
        "What do you notice in the loglog scale plot? Comment about this."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Hm-VR0w6LMaq"
      },
      "id": "Hm-VR0w6LMaq"
    },
    {
      "cell_type": "markdown",
      "id": "f1307cc5-e3df-4255-a0ca-19830584b05a",
      "metadata": {
        "id": "f1307cc5-e3df-4255-a0ca-19830584b05a"
      },
      "source": [
        "## **<font color=\"darkred\">QUESTION 3</font>**\n",
        "\n",
        "**Study the case of signal fraction**: Consider that we can better analyse the candidate events and we have a handle to purify the signal fraction, $f_S$, in the observed sample $n_{\\rm obs}$:\n",
        "$$n_{obs}= \\underbrace{f_S\\times n}_{S}  + \\underbrace{(1-f_S) \\times n}_{B}$$\n",
        "\n",
        "How does $\\sigma_\\mu = \\sqrt{{\\mathbb V}[\\mu]}$ evolves with $f_S$ for $n=10,000$. Plot the variation of $\\sigma_\\mu$ in log-log scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fbc08cd-7239-460a-9896-bc199ad34725",
      "metadata": {
        "metadata": {},
        "id": "1fbc08cd-7239-460a-9896-bc199ad34725"
      },
      "outputs": [],
      "source": [
        "### Type your code here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b58160e",
      "metadata": {
        "id": "6b58160e"
      },
      "source": [
        "Comment about the equivalent signal fraction $f_S$ needed to reach $\\sigma_{\\mu}=10^{-1}$ with $n=10,000$ and the $\\alpha$ scaling parameter required for reaching the same sensitivity. Why larger signal fraction leads to smaller uncertainty in μ?\n",
        "\n",
        "Compute $\\sigma_\\mu$ for 3 different configurations:\n",
        "1. S = 100, n = 10,000, fS = 0.01\n",
        "2. S = 1,000, n = 10,000, fS = 0.10\n",
        "3. S = 10,000, n = 1,000,000, fS = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36e0c454-6db1-4fbc-ba69-a94e1e9644a8",
      "metadata": {
        "metadata": {},
        "id": "36e0c454-6db1-4fbc-ba69-a94e1e9644a8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca0d4715",
      "metadata": {
        "metadata": {},
        "id": "ca0d4715"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9fbe805",
      "metadata": {
        "metadata": {},
        "id": "e9fbe805"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XzUAQWEULUlm"
      },
      "id": "XzUAQWEULUlm"
    },
    {
      "cell_type": "markdown",
      "id": "104c5e69-0b89-4e98-bcc3-b89efa518e7f",
      "metadata": {
        "id": "104c5e69-0b89-4e98-bcc3-b89efa518e7f"
      },
      "source": [
        "## **<font color=\"darkred\">QUESTION 4</font>**\n",
        "\n",
        "Consider now $f_S=0.2$ is fixed and get $\\sigma_\\mu$ as a function of total number of events $n$ between 2,600 and 250,000.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b00e0d9b-25f2-4834-b9b1-85760146e512",
      "metadata": {
        "metadata": {},
        "id": "b00e0d9b-25f2-4834-b9b1-85760146e512"
      },
      "outputs": [],
      "source": [
        "### Type your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81d49e4d-4be1-4bbe-b1de-15a02503b521",
      "metadata": {
        "id": "81d49e4d-4be1-4bbe-b1de-15a02503b521"
      },
      "source": [
        "Why, with still only 20% of total number of events only is it possible to reach a 1% precision with a large 250,000 events dataset?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba0e0811-8047-46de-b118-7f797b82347b",
      "metadata": {
        "id": "ba0e0811-8047-46de-b118-7f797b82347b"
      },
      "source": [
        "# <font color=\"darkred\">**Part 2: Taking into account systematic uncertainties**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97e02f53-ce3c-4942-b3ee-eea56ab13b41",
      "metadata": {
        "id": "97e02f53-ce3c-4942-b3ee-eea56ab13b41"
      },
      "source": [
        "On top of potential errors with respect to expectations due to random statistical fluctuations, our expectation on total number of signal and background events could have been wrongly calculated for several reasons. In the following of this section, we consider systematic uncertainties associated to signal and background number of events.\n",
        "\n",
        "To estimate potential errors on our determination of $\\mu$ we will estimate associated uncertainties on $S$ and $B$.\n",
        "\n",
        "Physically the total number of signal events $S$ can we written as:\n",
        "\n",
        "$$S = {\\cal L}\\times\\sigma_S\\times\\varepsilon_S$$\n",
        "\n",
        "and for the total number of brackground events $B$:\n",
        "\n",
        "$$B = {\\cal L}\\times\\sigma_B\\times\\varepsilon_B$$\n",
        "\n",
        "where,\n",
        "- ${\\cal L}$ is the integrated luminosity (flux) of particles participating to interaction process within the detector,\n",
        "- $\\varepsilon_S$ is the signal detection efficiency,\n",
        "- $\\varepsilon_B$ the background detection efficiency,\n",
        "- $\\sigma_S$ sthe signal cross section,\n",
        "- $\\sigma_B$ the background cross section.\n",
        "\n",
        "${\\cal L}$, $\\varepsilon_S$ and $\\varepsilon_B$ are not fully determined and are subject to repsective relative uncertainties:\n",
        "- $\\delta {\\cal L} = 1\\%$\n",
        "- $\\delta \\varepsilon_S = 5\\%$\n",
        "- $\\delta \\varepsilon_B = 2\\%$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05d257ce-9c2e-4ff5-b9f5-742e9e55b1cb",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "05d257ce-9c2e-4ff5-b9f5-742e9e55b1cb"
      },
      "source": [
        "## **<font color=\"darkred\">QUESTION 5</font>**\n",
        "\n",
        "Using linear error propagation on $\\mu$, with the above uncertain parameters, determine the total uncertainty $\\sigma_\\mu$ on $\\mu$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "885aa1d5",
      "metadata": {
        "id": "885aa1d5"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below a functor is defined. A functor is an object, when called for the first time defines a function with specific internal parameters provided. The instant of the object thus created is specific with the values provided for the creation. Then this object can be used as a regulard function thanks to the `__call__` attribute of the class."
      ],
      "metadata": {
        "id": "5ArAMZimLgyT"
      },
      "id": "5ArAMZimLgyT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44aacab9",
      "metadata": {
        "id": "44aacab9"
      },
      "outputs": [],
      "source": [
        "class δμFunctor():\n",
        "    def __init__(self, δε_S=0.1, δε_B=0.1, δL=0.1):\n",
        "        self.δε_S, self.δε_B, self.δL = δε_S, δε_B, δL\n",
        "\n",
        "    def Jacobian(self, S, B):\n",
        "        n = S+B\n",
        "        ### WRITE THE CORRECT VALUES THERE:\n",
        "        return np.array([..., ..., ..., ...])\n",
        "        ###\n",
        "\n",
        "    def getUncertainty(self, S, B):\n",
        "        uncertainties = np.array([1/np.sqrt(S+B), self.δε_S, self.δε_B, self.δL])\n",
        "        return np.sqrt(sum(self.Jacobian(S, B)**2 * uncertainties**2))\n",
        "\n",
        "    __call__ = getUncertainty"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1abb8ffe",
      "metadata": {
        "id": "1abb8ffe"
      },
      "source": [
        "Calling the `δμFunctor` in the cell below should run floawlessly and gives `δμ=0.234`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94034788",
      "metadata": {
        "id": "94034788"
      },
      "outputs": [],
      "source": [
        "# We define a specific δμ functino here with provided uncertainties\n",
        "δμ = δμFunctor(δε_S=.05, δε_B=.02, δL=.01)\n",
        "# Here \"function\" δμ is called with S and B values and compute the total uncertainty\n",
        "# for values of S and B provided.\n",
        "print(δμ(100, 2_500))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now define a usefull printing command to play with $\\alpha$ and $f_S$ values instead of $S$ and $B$."
      ],
      "metadata": {
        "id": "icpnfp25MkZx"
      },
      "id": "icpnfp25MkZx"
    },
    {
      "cell_type": "code",
      "source": [
        "δμ_printer = lambda n, α, fS: print(f\"{n = },\\t{α = },\\t{fS = }\\t--->\\tδμ = {δμ(n*α*fS, n*α*(1-fS)):.3f}\")\n",
        "δμ_printer(n=10_000, α=1.0, fS=.1)"
      ],
      "metadata": {
        "id": "Mw65FbKzMaf4"
      },
      "id": "Mw65FbKzMaf4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b2ad484a",
      "metadata": {
        "id": "b2ad484a"
      },
      "source": [
        "What is better?\n",
        "1. 10,000 events with 200 events of signal?\n",
        "2. 200 events with 150 events of signal?\n",
        "3. 100 events with 99 events of signal?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "455181bb",
      "metadata": {
        "id": "455181bb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "c10015c8",
      "metadata": {
        "id": "c10015c8"
      },
      "source": [
        "Comment on these computations?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d4a7288-7015-48c0-a945-abfbcad0c17d",
      "metadata": {
        "id": "0d4a7288-7015-48c0-a945-abfbcad0c17d"
      },
      "source": [
        "---\n",
        "# <font color=\"darkred\">**Study of systematics with a trained BDT on test samples**</font>\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we will not spend time on developping and tuning Machine Learning algorithm. This part of the job is addressed within ML lectures and practice sessions. Here we are interested to use these algorithms to estimate uncertainties and to compute final outputs on statistical signifiance such as the AMS (average median significance) or the $\\delta\\mu$ uncertainty.\n",
        "\n",
        "The initial dataset, model and computations are provided within a specific module provided for this notebook. This module is called `systLib`. It's role is to assist you on your path to estimate statistical and systematical uncertainties on a quantitiy of interest such as the $\\mu$ parameter.\n",
        "\n",
        "If you are curious you can look in at the code provided in the `systLib.py` file once dowloaded. It basically creates classes around codes extracted from ML practice session from David."
      ],
      "metadata": {
        "id": "mDM0jTmGGRhQ"
      },
      "id": "mDM0jTmGGRhQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will need to download a data files from your professor's Google Drive. In order to do so, you need to authenticate with you Google account, so when executing the following cell, you will be prompted at some point to enter a verification code. In order to get it, follow the instructions given in the popup windows once executing next cell."
      ],
      "metadata": {
        "id": "DJzRztqZAZpj"
      },
      "id": "DJzRztqZAZpj"
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive2.auth import GoogleAuth\n",
        "from pydrive2.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "HA0BBwHYBjib"
      },
      "id": "HA0BBwHYBjib",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking local content before downloading files\n",
        "!ls -rtlh"
      ],
      "metadata": {
        "id": "ta8MusL08q8I"
      },
      "id": "ta8MusL08q8I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the training set from Google Drive\n",
        "download = drive.CreateFile({'id': '1Gtw9_i4e4XuDZnxLbVct7AqSQYaB7ZST'})\n",
        "download.GetContentFile('data.csv.gz')\n",
        "# Download systLib module\n",
        "download = drive.CreateFile({'id': '1-4ejSiPc2Y4tISgsr6zghEbCPiGe9x1m'})\n",
        "download.GetContentFile('systLib.py')"
      ],
      "metadata": {
        "id": "ZksbKRFoD5oU"
      },
      "id": "ZksbKRFoD5oU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cheking data file is now accessible in colab, and it's timestamp\n",
        "!ls -rtlh"
      ],
      "metadata": {
        "id": "qriZB_vmEaNr"
      },
      "id": "qriZB_vmEaNr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see 2 new files:\n",
        "- `data.csv.gz`\n",
        "- `systLib.py`\n",
        "\n",
        "Otherwise, something went wrong on the way. In that latter case restart the previous steps."
      ],
      "metadata": {
        "id": "g1Xd7KyvDBX4"
      },
      "id": "g1Xd7KyvDBX4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import a module, `systLib`, which has been specifically developped for this notebook."
      ],
      "metadata": {
        "id": "RbBmfDBiLb9v"
      },
      "id": "RbBmfDBiLb9v"
    },
    {
      "cell_type": "code",
      "source": [
        "from systLib import DataObject, Model, make_systematic_datasets # type: ignore"
      ],
      "metadata": {
        "id": "O-I9_1xf42BY"
      },
      "id": "O-I9_1xf42BY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this module we will use the following objects and function:\n",
        "- `DataObject` is a class which whose instantance contains pre-processed data. It is used to get train and test datasets.\n",
        "- `Model` is a class to be used for training models on training data and evaluating them on testing datasets.\n",
        "- `make_systematics_datasets` is a function to be used to compute effect of systematics on test dataset.\n"
      ],
      "metadata": {
        "id": "Sx-RhYJRLwLa"
      },
      "id": "Sx-RhYJRLwLa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the same dataset which was introduced in the first ML practice session with David. It is a Monte Carlo simulation of a Higgs boson decay into two W bosons. This is a quite common decay mode of the Higgs boson (21%). We are then interested in the process where two W bosons ($W^+$ and $W^-$) then decay into two (oppositely) charged leptons and and two neutrinos. The neutrinos leave the detector being undetected and give rise to imbalance in energy conservation. This channel study therefore introduces the quantities related to the two charged leptons and missing transverse energy, MET. The reduced dataset which is here provided contain only the following observables:\n",
        "- the two charged leptons transverse momenta and \"angles\" ($\\eta$ and $\\phi$)\n",
        "- the two missing transverse energy angles ($\\eta$ and $\\phi$)\n",
        "\n",
        "totalizing 6 features as in ML practice session #1.\n",
        "\n"
      ],
      "metadata": {
        "id": "Uf4axSrrPzav"
      },
      "id": "Uf4axSrrPzav"
    },
    {
      "cell_type": "code",
      "source": [
        "data = DataObject()\n",
        "ds_train = data.get_train_dataset()\n",
        "ds_test = data.get_test_dataset()"
      ],
      "metadata": {
        "id": "IUwN5rRcQisj"
      },
      "id": "IUwN5rRcQisj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We assume the following:\n",
        "- a luminosity systematic error $\\delta {\\cal L} = 1\\%$\n",
        "- a signal systematic normalization error $\\delta \\varepsilon_S = 5\\%$\n",
        "- a background systematic normalization error $\\delta \\varepsilon_B = 2\\%$\n",
        "- a detector response systematic error $\\delta R = 2\\%$\n"
      ],
      "metadata": {
        "id": "szp8-KQ-MonQ"
      },
      "id": "szp8-KQ-MonQ"
    },
    {
      "cell_type": "code",
      "source": [
        "ds = make_systematic_datasets(ds=ds_test, δε_S=.05, δε_Β=.04, δL=.01, δR=.02)"
      ],
      "metadata": {
        "id": "o_BQKO6ZNZwL"
      },
      "id": "o_BQKO6ZNZwL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the datasets have been prepared, we will train some models such as LightGBM, XGBoost, scikit-learn HistGradientBoosting and a RandomForest model."
      ],
      "metadata": {
        "id": "v77GUL3fHKxq"
      },
      "id": "v77GUL3fHKxq"
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of 4 models to train and test for systematics\n",
        "from lightgbm import LGBMClassifier as LGB\n",
        "from xgboost.sklearn import XGBClassifier as XGB\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier as HGB\n",
        "from sklearn.ensemble import RandomForestClassifier as RF"
      ],
      "metadata": {
        "id": "Bc8B-2VE6cK6"
      },
      "id": "Bc8B-2VE6cK6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the simple way to define the model and train it on a train dataset and plot it's AMS performance on a test dataset."
      ],
      "metadata": {
        "id": "c_v-aHNpHigs"
      },
      "id": "c_v-aHNpHigs"
    },
    {
      "cell_type": "code",
      "source": [
        "m = Model(classifier=HGB)\n",
        "m.train(ds_train)\n",
        "m.plot(ds_test);"
      ],
      "metadata": {
        "id": "pwrgkD3Q8Aaq"
      },
      "id": "pwrgkD3Q8Aaq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **<font color=\"darkred\">QUESTION 6</font>**\n",
        "\n",
        "From the above code examples, try different models such as:\n",
        "- `HGB`\n",
        "- `LGB`\n",
        "- `XGB`\n",
        "- `RF`\n",
        "\n",
        "What do you notice? Are there models better than others? Is the training time comparable?"
      ],
      "metadata": {
        "id": "yzys4mZ5PMTT"
      },
      "id": "yzys4mZ5PMTT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **<font color=\"darkred\">QUESTION 7</font>**\n",
        "\n",
        "Loop on the models and extract the AMS, average median significance which we have covered at the end of Lecture 4.\n",
        "The AMS definition is:\n",
        "\n",
        "$${\\rm AMS} = \\sqrt{2\\left(\\left(S+B\\right)\\log\\left(1+\\frac{S}{B}\\right)-S\\right)}$$\n",
        "\n",
        "This quantity is accessible through a call to `Model.get_ams(data_set, scores)` method."
      ],
      "metadata": {
        "id": "TBd7eFqSTlSO"
      },
      "id": "TBd7eFqSTlSO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill the code below to perform the requested actions:"
      ],
      "metadata": {
        "id": "XjzzbAA2NMtu"
      },
      "id": "XjzzbAA2NMtu"
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(0.4, 0.9, 100)\n",
        "models = [HGB, LGB, XGB]\n",
        "ams_models = []\n",
        "ams_max_models = []\n",
        "for model in models:\n",
        "  m = ...\n",
        "  ...\n",
        "  _, ams = m.evaluate(ds_test, x)\n",
        "  ams_models.append(ams)\n",
        "  ams_max_models.append(m.getArgMaxAMS(ds_test))"
      ],
      "metadata": {
        "id": "z7wgnerkNsb5"
      },
      "id": "z7wgnerkNsb5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['HGB', 'LGB', 'XGB']\n",
        "for ams, label in zip(ams_models, labels):\n",
        "  plt.plot(x, ams, label=label)\n",
        "plt.grid(ls='--')\n",
        "plt.legend()\n",
        "plt.ylabel('AMS')\n",
        "plt.xlabel('score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SdhTdcyfOcs1"
      },
      "id": "SdhTdcyfOcs1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ams_max_models"
      ],
      "metadata": {
        "id": "5SFQ3rhTEFMh"
      },
      "id": "5SFQ3rhTEFMh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can investigate shapes of signal and background in two datasets with `Model.compare` method"
      ],
      "metadata": {
        "id": "cWaVPJ-wFya4"
      },
      "id": "cWaVPJ-wFya4"
    },
    {
      "cell_type": "code",
      "source": [
        "m.compare(ds['test'], ds['sig'], range=(0.5, 1), bins=30, density=False)"
      ],
      "metadata": {
        "id": "BC5ZHwfPGDCk"
      },
      "id": "BC5ZHwfPGDCk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to compute for different score values the δμ of each systematic\n",
        "\n",
        "For computation time efficiencies it is faster to get for each systematic the computation for all the scores at the same time.\n",
        "\n",
        "This means we will get a data structure which is a list of 4 lists of δμ  for each score values.\n"
      ],
      "metadata": {
        "id": "ZNyALnJHMk5q"
      },
      "id": "ZNyALnJHMk5q"
    },
    {
      "cell_type": "code",
      "source": [
        "δμ_stat = []\n",
        "n, S, _ = m.get_nSB(ds_test, x)\n",
        "eps = 1e-12\n",
        "for _n, _S in zip(n, S):\n",
        "  δμ_stat.append(np.sqrt(_n)/(_S+eps))  # numerical trick to avoid division by 0\n",
        "δμ = []\n",
        "for d in ds.values():\n",
        "  _, S, B = m.get_nSB(d, x)\n",
        "  δμ.append([(_n-_B)/(_S+eps) for _n, _S, _B in zip(n ,S, B)])"
      ],
      "metadata": {
        "id": "sUgUE1kVwev8"
      },
      "id": "sUgUE1kVwev8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "δμ is a list of 4 lists of δμ_syst values for score values\n",
        "\n",
        "To compute the total δμ_tot = sqrt(sum(δμ_syst_i^2)) we need to then \"transpose\" this list of lists.\n",
        "\n",
        "Here are 3 methods:\n",
        "\n",
        "1. Using python list comprehension:\n",
        "\n",
        "`>>> δμT = [[row[i] for row in μ] for i in range(len(μ[0]))]`\n",
        "\n",
        "2. Using map to apply a list grouping of each element of the 4 lists\n",
        "\n",
        "`>>> δμT = list(map(list, zip(*μ)))`\n",
        "\n",
        "\n",
        "3. Using a numpy array conversion from list of list to 2D array, transpose and back to list\n",
        "\n",
        "`>>> δμT = np.asarray(δμ).T.tolist()`\n"
      ],
      "metadata": {
        "id": "Iy4IbWlKMyFT"
      },
      "id": "Iy4IbWlKMyFT"
    },
    {
      "cell_type": "code",
      "source": [
        "δμT = np.asarray(δμ).T.tolist()\n",
        "μ0 = 1\n",
        "δμ_syst_total = [sum([(δμ_ix-μ0)**2 for δμ_ix in δμ_x]) for δμ_x in δμT]\n",
        "δμ_tot = [np.sqrt(u**2 + y**2) for u, y in zip(δμ_stat, δμ_syst_total)]"
      ],
      "metadata": {
        "id": "OUb912AEMwri"
      },
      "id": "OUb912AEMwri",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We finish by plotting the δμ's vs. score values:"
      ],
      "metadata": {
        "id": "ac44qDdjNL9d"
      },
      "id": "ac44qDdjNL9d"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(x, δμ_stat, label='stat')\n",
        "plt.plot(x, δμ_syst_total, label='syst')\n",
        "plt.plot(x, δμ_tot, label='total')\n",
        "plt.legend(facecolor='white')\n",
        "plt.xlabel('score')\n",
        "plt.ylabel(r'$\\delta{\\mu}$')\n",
        "plt.grid(ls='--')\n",
        "plt.axis([min(x), max(x), 0, 0.5])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_etbJVIBNKjJ"
      },
      "id": "_etbJVIBNKjJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **<font color=\"darkred\">QUESTION 8</font>**\n",
        "\n",
        "Based on the preceding code define a function to compute the statistical and systematic uncertainties for a given model.\n",
        "\n",
        "Your job is to build up a table of systematic uncertainties for each model. This table should contain:\n",
        "\n",
        "- a row per model: LGB, HGB, XGB, RF (last one if possible)\n",
        "- columns: δμ_stat, δμ_sig, δμ_bkg, δμ_lumi, δμ_det, δμ_syst_tot, δμ_tot,\n",
        "\n"
      ],
      "metadata": {
        "id": "RdPGuFD8LloW"
      },
      "id": "RdPGuFD8LloW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "436b6108",
      "metadata": {
        "id": "436b6108"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "d7e869e7",
      "metadata": {
        "id": "d7e869e7"
      },
      "source": [
        "---\n",
        "# Footnotes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<a name=\"cite_note-1\"></a>[[^1]](#cite_ref-1) In reference to the Isaac Asimov's novel *Franchise* (1955). In this short story, the outcome of the 2008 US Presidential election is determined by the choice made by a single representative of the entire electorate, an average citizen. The scientific article which first used this name for this reference dataset is  [[arXiv: 1007.1727](https://arxiv.org/pdf/1007.1727.pdf)]. The term is now often used to describe this peculiar setup. (Go back to [citation mark](#back_to_citation_1))"
      ],
      "metadata": {
        "id": "LLugTswLFcMO"
      },
      "id": "LLugTswLFcMO"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}