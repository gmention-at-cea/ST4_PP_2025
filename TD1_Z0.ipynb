{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FUS5JEfRKtH-"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmention-at-cea/ST4_PP_2024/blob/main/TD1_Z0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "iTc_P6gZEm8n"
      },
      "source": [
        "# How many neutrino families do exist?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2iaNe6XFTcd"
      },
      "source": [
        "# Before you start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_YeOQ7_FPKH"
      },
      "source": [
        "To do this homework, you have to download it on your own \"Google Colaboratory\" space (see in the \"File\" menu, \"save a copy in your Google Drive\", you will have to create a gmail account if you don't already have one) so that you can modify it as you wish. You need to train with Google Colaboratory as you will be using it for the Challenging Week (Enseignement d'Intégration).\n",
        "\n",
        "Once you did indeed save this notebook on your own Google drive, you can modify it and save it again. If you corrupt anything by mistake, you have automatic recording of previous version which you can still acess to get back. Anyway you can still start from scratch with the default notebook if you experience really weird things with your current version.\n",
        "\n",
        "On the Help menu above, (menu \"Aide\" in French) you can switch the language of the interface. For this lectures I set it to English."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "lQ0zMe7sEm8o"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "The purpose of this exercise is to review error propagation in two different ways.\n",
        "First we will use the Delta method, propagating errors at first order only with a Taylor expansion as seen during the lectures. As a second, more general approach, we will address this topic through random sampling (Monte Carlo simulation) to assess the mean and standard deviation.\n",
        "\n",
        "We propose in this exercise to re-establish a landmark result of the fit of the $Z^0$ boson width which determined the number of families of neutrinos which interact with it.\n",
        "\n",
        "As we have seen during the lectures, up to now, we have identified 3 generations of fermions for leptons. At the beginning of the 90s the tau neutrino, $\\nu_\\tau$, has not yet been observed (neither the top quark). Thus only 2 families of neutrino were discovered.\n",
        "\n",
        "The LEP, the Large Electron-Positron collider, performed a scan in energies around the $Z^0$ mass of $91\\textrm{ GeV}$, between 88 GeV and 94 GeV. This colliding machine, the predecessor of the LHC, was colliding electron, $e^-$, with positron $e^+$. It performed a lot of precise measurements of  Standard Model parameters among which the $Z^0$ width $\\Gamma_Z$ and mass $m_Z$.\n",
        "\n",
        "<figure style=\"text-align:center\">\n",
        "<img src=\"https://github.com/gmention-at-cea/ST4_PP_2024/blob/main/Z0plot.png?raw=1\" width=\"40%\">\n",
        "    <figcaption><span  style=\"font-weight:bold; color:darkred\">Fig. 1</span> | Combined LEP $Z^0$ measurements. The y-axis reports the cross section of the interaction of an electron and a positron through the exchange of a $Z^0$ boson. The cross section relates to an effective area proportional to the interaction strength within the particles. The product of the cross section by the flux of incident particles give the number of interactions per unit of time. The x-axis display the invariant mass of the 2 outgoing fermions as seen during the lectures. We see a resonance at the mass of the $Z^0$ particle produced in this interaction process.\n",
        "    </figcaption>\n",
        "</figure>\n",
        "\n",
        "In measuring the $Z^0$ cross section, experimentalists accessed to the resonance width of an intermediate state such as the $Z^0$ $e^++e^-\\longrightarrow Z^0 \\longrightarrow f+\\bar{f}$ is the sum of the widths of all the possible final states $f+\\bar{f}$:\n",
        "\n",
        "$$\\Gamma_Z = \\sum_{\\rm f}\\Gamma_{\\rm ff} = \\Gamma_{\\rm ee} + \\Gamma_{\\mu\\mu}+\\Gamma_{\\tau\\tau}+\\Gamma_{\\rm had}+\\Gamma_{\\nu\\nu}+\\ldots$$\n",
        "\n",
        "The universaility of the $Z^0$ couping with leptons state (which has been experimentally verified) establish that the 3 charged leptons have the same coupling to the $Z^0$ and theretore $\\Gamma_{\\rm ee}=\\Gamma_{\\mu\\mu}=\\Gamma_{\\tau\\tau}$. We choose to note this common quantity $=\\Gamma_{\\rm 2\\rm lep}$. The same applies among the neutrinos. If we think we have included all the potential decays of the $Z^0$, we end up with:\n",
        "$$\\Gamma_Z = 3\\Gamma_{\\rm 2lep}+\\Gamma_{\\rm had}+N_{\\nu}\\Gamma_{\\nu\\nu}$$\n",
        "However, neutrinos escape the detection, they interact too weakly. They contribute to what is called the \"invisible width\" of the $Z^0$, $\\Gamma_{\\rm inv}$. If notheing else than the neutrinos contribute to the unseen decays of the $Z^0$, then $\\Gamma_{\\rm inv}=N_{\\nu}\\Gamma_{\\nu\\nu}$ where $N_{\\nu}$ is the number of neutrino families which couple to $Z^0$ in interactions.\n",
        "The cross section of the interaction process when the $Z^0$ decays to hadrons at the $m_Z$ energy give the following formula:\n",
        "$$\\sigma_{\\rm had}^0=\\frac{12\\pi(\\hbar c)^2}{m_Z^2}\\frac{\\Gamma_{\\rm ee}\\Gamma_{\\rm had}}{\\Gamma_Z^2}$$\n",
        "Experimentally it is often better to proceed in relative quantities, or ratios, to cancel common uncertainties in two quantities, therefore the LEP experiments have provided the measured ratio $R_{\\rm 2lep}^0=\\frac{\\Gamma_{\\rm had}}{\\Gamma_{\\rm 2lep}}$ ($R_{\\rm 2lep}^0$ is indicated as $R_{\\rm l}^0$ in the table but to avoid here confusion between letter \"l\" as in \"letter\" and the digit \"1\" as in \"1, 2, 3,...\" we write $R_{\\rm 2lep}^0$ in this notebook because of the font used by default here) and the predicted ratio $\\frac{\\Gamma_{\\nu\\nu}}{\\Gamma_{\\rm 2lep}}=1.99125\\pm 0.00083$.\n",
        "<figure style=\"text-align:center\">\n",
        "<img src=\"https://github.com/gmention-at-cea/ST4_PP_2024/blob/main/Z0_LEP_combined_measurement.png?raw=1\" width=\"80%\">\n",
        "    <figcaption><span  style=\"font-weight:bold; color:darkred\">Tab. 1</span> | Combined LEP $Z^0$ measurements, their uncertainties and correlation matrix. Note that the correlation matrix is symmetric by definition and only subdiagonal elements have been displayed. It should be understood that the reader complete the upper part of this matrix by symmetry to the diagonal.\n",
        "    </figcaption>\n",
        "</figure>\n",
        "\n",
        "In Table 1 we provide the standard deviations (the uncertainties $\\sigma_i$) on the parameters and the correlation matrix, $(C_{i,j})$. You have to know the relation between the covariance matrix $(V_{i,j})$, the correlation matrix $(C_{i,j})$ and the uncertainties ($\\sigma_i$). The diagonal of the covariance matrix contains the variance of each parameter $V_{i,i}$. The square root of the diagonal gives thus the standard deviations of the parameters $\\sigma_i=\\sqrt{V_{i,i}}$. Then the correlation matrix is given by:\n",
        "\n",
        "$$C_{i,j} = \\frac{V_{i,j}}{\\sqrt{V_{i,i} V_{j,j}}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4M2PjkdJ4Zc"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHwi62vXFrY5"
      },
      "source": [
        "# Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAdgY4qmG4JH"
      },
      "source": [
        "**Generic rules**\n",
        "In the work bellow, you we see sections with **Questions**.\n",
        "For each question you are provided with a `TEXT` cell for explanations of the work to do and a `CODE`. We already provide some code framework to guide you on what we expect. For instance if there are imported functions of packages, you are supposed to use them to solve this question.\n",
        "\n",
        "We also put some `xxx` in these code cells. These are leftover places where you should fill with the correct code to answer the question.\n",
        "\n",
        "To make the whole notebook work properly, you have to execute the cells in sequence once you answered the previous cells. Some of them depends on previous ones, either from the answer you provided or the packages which were loaded. If you experience difficulties in executing the notebook and things seem corrupted, you can restart the Kernel (the engine executing the notebooks in the top menu \"Runtime\").\n",
        "\n",
        "*One last thing: you can at will add your own cells of `CODE` or `TEXT` to do you tests. You can also have another notebook to perform some computations you would like to do.*\n",
        "\n",
        "So, that's it to start. **You are supposed to fill the `xxx` below.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "VSsKNAKBEm8p"
      },
      "source": [
        "## Question 1\n",
        "\n",
        "Express the number of neutrinos $N_{\\nu}$ as a function of the measured and predicted quantities: $\\frac{\\Gamma_{\\nu\\nu}}{\\Gamma_{\\rm 2lep}}$ (`Rnu_2lep`), $R_{\\rm 2lep}^0$ (`R_had2lep`), $m_Z$ (`mZ`) and $\\sigma_{\\rm had}^0$ (`sig_had`). For numerical evaluation, it is often useful to remember that $\\hbar c \\simeq  1973{\\rm\\; eV}\\;\\,\\mathring{\\hspace{-1.5mm}A} \\simeq 200{\\rm\\; MeV\\,fm}$. After expressing the mathematical relation, implement it in python code below. Beware of units..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "CdjaeCtvEm8p"
      },
      "source": [
        "from numpy import sqrt, pi, NaN\n",
        "xxx = NaN # I define xxx to NaN (Not a Number) such that you can still execute\n",
        "# the cell. It will execute but won't provide the answer.\n",
        "\n",
        "def Nnu(Rnu_2lep,mZ,sig_had,R_had2lep):\n",
        "    hbarc = 1973 # in eV Angström; 1 Angström = 1e-10 meter.\n",
        "    return xxx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqF9bLEpJlIP"
      },
      "source": [
        "Don't forget to check that your function definition is working with a few extra `CODE` cells you might insert here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVTbM16xJ92x"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "WuayadNvEm8u"
      },
      "source": [
        "## Question 2\n",
        "\n",
        "Write the total correlation matrix, $C$, for the predicted ratio $\\frac{\\Gamma_{\\nu\\nu}}{\\Gamma_{\\rm 2lep}}$ as well as the all variables in the table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "3keDuRaWEm8v"
      },
      "source": [
        "# For a beautiful display of tables, matrices etc. and to keep track of parameter\n",
        "# names we propose use pandas package with its DataFrame Function\n",
        "\n",
        "from pandas import DataFrame as df\n",
        "from numpy import NaN\n",
        "xxx = NaN\n",
        "\n",
        "# => Replace the xxx (NaN) values by the correct values from the table and the text\n",
        "\n",
        "# Names of the variables\n",
        "names = ( 'R_nu2lep' , 'mZ' , 'GammaZ' , 'sigma_had' , 'R_had2lep' , 'AFB' )\n",
        "\n",
        "# Replace the \"xxx\" values below by what you think should be the correct values\n",
        "\n",
        "# Correlation matrix - column and row orders follow names list order\n",
        "C = df(\\\n",
        "    [[  xxx  ,  xxx  ,  xxx  ,  xxx   , xxx  ,  xxx  ],\\\n",
        "     [  xxx  ,  1    , -0.023, -0.045,  0.033,  0.055],\\\n",
        "     [  xxx  , -0.023,  1    , -0.297,  xxx  ,  0.033],\\\n",
        "     [  xxx  , -0.045,  xxx  ,  1    ,  0.183,  0.006],\\\n",
        "     [  xxx  ,  0.033,  0.004,  0.183,  1    ,  xxx  ],\\\n",
        "     [  xxx  ,  0.055,  0.033,  0.006, -0.056,  1    ],\\\n",
        "    ], columns=names , index=names )\n",
        "\n",
        "# Standard deviation\n",
        "s = df( [xxx, .021, .0023, .037, .025, xxx] , names )\n",
        "\n",
        "mu = df( [xxx, 91.1875, 2.4952, xxx, 20.767, 0.0171] , names )\n",
        "\n",
        "# to access to the values of a DataFrame object, use \".values\"\n",
        "s.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z80rF3blKNrk"
      },
      "source": [
        "Again you can add your own subsection here with `###` and a subtitle. This way you can put some `CODE` cells to test your solution and check it is working. You can keep this `CODE` cells for memory. Just you can fold in/out this subsection within the Question for better readability with the little arrow (*right* = folded or *down* = unfolded) after you click outside your cell editing. You can access to the Table of Content either on the left through the bullet list icon or within the \"View\" menu on the top. Click back on any sectino you are interested in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUS5JEfRKtH-"
      },
      "source": [
        "### Put here your own title\n",
        "And write you own `TEXT` cells and `CODE` cells to work and check your solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUsjwl4EK3-Y"
      },
      "source": [
        "def ff(x):\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtKNSBIZK7u_"
      },
      "source": [
        "print(ff(2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOqgkrrdL4k-"
      },
      "source": [
        "Cool!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j51qiN8MxmI"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "zTprMe87Em8z"
      },
      "source": [
        "## Question 3\n",
        "\n",
        "Write the covariance matrix, $V$, from the above values in the table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "u0bgr1pKEm80"
      },
      "source": [
        "from numpy import diag, diagflat\n",
        "# for instance, it could be useful to consider\n",
        "# diagflat(s.values) OR\n",
        "# diag((s.values()).ravel())\n",
        "# and the matrix product operator \"@\"\n",
        "\n",
        "V = df( xxx ,\\\n",
        "       columns=names,index=names)\n",
        "V"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbOnEeU7MJPV"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "C60eMnxaEm83"
      },
      "source": [
        "We propose in the following to assess the number of neutrinos from the mean values of the parameters given above and to assess the uncertainty on the determined number of neutrinos which couple to the $Z^0$ by first order error propagation from Taylor expansion around the mean."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "Rkdzb2IbEm84"
      },
      "source": [
        "from numpy import take, zeros, arange"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "IS4JaOu_Em87"
      },
      "source": [
        "# To get particular values with specified indicies spread apart,\n",
        "# use the function \"take\" or also \"ix_\" from the numpy package\n",
        "# for instance to get first, second fourth and fifth values of mu write:\n",
        "# idx = [0,1,3,4]\n",
        "# >> take(mu.values,idx) # will do the job.\n",
        "# >> mu.values[ix_(idx)] # will also do the job\n",
        "# Beware however that both solutions do not give the same output. One is the transpose of the other.\n",
        "from numpy import transpose as T\n",
        "from numpy import ix_\n",
        "\n",
        "# Compute the numerical derivative for Nnu function\n",
        "# with respect to each of the parameters\n",
        "# Use a typical definition as (f(x+h) - f(x-h)) / (2*h)\n",
        "\n",
        "# scale factor for numerical derivative: scale = 1 means 1 sigma step forward and\n",
        "# backward around central value\n",
        "scale = 1\n",
        "\n",
        "idx = [0,1,3,4] # indicies of the parameters in which we are interested in\n",
        "plus = mu.values[ix_(idx)] + diagflat(take(scale*s.values,idx))\n",
        "minus = mu.values[ix_(idx)] - diag(take(scale*s.values,idx).ravel())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "o6dGfNbYEm8-"
      },
      "source": [
        "We have seen in the lecture that $\\sigma^2_y = \\left(\\frac{{\\rm d}y}{{\\rm d}x}\\right)^2\\sigma^2_x$. When $y$ is a vector $(y_1,\\ldots,y_n)$, the natural generalization is\n",
        "\n",
        "$${\\rm Cov}[y] = J\\, {\\rm Cov}[x]\\, J^\\prime$$\n",
        "\n",
        "where $J^\\prime$ is the transposed matrix of $J$ with $J_{i,k}=\\left.\\frac{\\partial y_i(x_1,\\ldots,x_n)}{\\partial x_k}\\right|_{x=x^\\star}$ where the derivatives are taken at a particular value, here the mean values of $x_1,\\ldots,x_n$.\n",
        "In indices notation, we have:\n",
        "\n",
        "$${\\rm Cov}[y_i,y_j] = \\sum_{k,k^\\prime=1}^n J_{i,k}\\, {\\rm Cov}[x_k,x_{k^\\prime}]\\, J_{j,k^\\prime}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "9ahQjiX7Em8_"
      },
      "source": [
        "# To call a function with several arguments whose values are in an array, use the\n",
        "# star operator to dispatch the array values into arguments for the function call\n",
        "# for instance to call Nnu defined above:\n",
        "# Nnu( *take( mu.values , [ 0 , 1 , 3 , 4 ] ) )\n",
        "\n",
        "\n",
        "# We compute now the Jacobian matrix with respect to the 4 variables\n",
        "J = zeros(4)\n",
        "for k in arange(0,4):\n",
        "    # beware of the row and column meaning of your plus and minus matrices\n",
        "    J[k] = xxx\n",
        "J"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "P79slPLJEm9D"
      },
      "source": [
        "# To get a submatrix: many ways among which:\n",
        "# you procceed in sequence in dimension after the other\n",
        "# M[idx,:][:,idx] or could be also\n",
        "# or produce a subselection mask with ix_[idx,idx]\n",
        "# M[ix_[idx,idx]]\n",
        "# subV = V.values[idx,:][:,idx]\n",
        "\n",
        "subV = V.values[ix_(idx,idx)]\n",
        "\n",
        "mu_Nnu = Nnu(*take(mu.values,idx))\n",
        "\n",
        "# Write the expression on the uncertainty in Nnu (standard deviation)\n",
        "# from what you developped just before\n",
        "\n",
        "sigma_Nnu = xxx\n",
        "\n",
        "print('Nnu = {:1.5}'.format(mu_Nnu),'± {:1.2}'.format(sigma_Nnu))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "_yRJnnS5Em9G"
      },
      "source": [
        "Do the correlation among variables play a crucial role in the final estimate on the uncertainty on $N_{\\nu}$?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "m9fK0pALEm9H"
      },
      "source": [
        "ratio_cor_to_no_cor = xxx / xxx\n",
        "print('ratio cor to no cor',ratio_cor_to_no_cor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcQGvTJ8Noa0"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "BAy1nmyuEm9K"
      },
      "source": [
        "## Question 4\n",
        "We propose to assess the mean and standard deviation of $N_{\\nu}$ through a Monte Carlo simulation. A Monte Carlo simulation is simply a numerical computation using random numbers.\n",
        "<br/>\n",
        "<br/>\n",
        "Assuming the distribution of all the variables are normal distributions, write a code providing a sample of all the random variables which enter in the definition of $N_{\\nu}$.\n",
        "For each of these sample simulation compute the $N_{\\nu}$ associated to the draws all the variables. Be carefull to take into account the correlations in this task. For this, compute the correlation matrix between the samples of $m_Z,\\ldots,A_{\\rm FB}^{0l}$. If you did properly the job you should find values from Table 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "c_DdNpZZEm9L"
      },
      "source": [
        "import pandas\n",
        "from numpy import random, isnan\n",
        "names = ('R_nu2lep','mZ','GammaZ','sigma_had','R_had2lep','AFB')\n",
        "nsim = 1000\n",
        "\n",
        "X = xxx\n",
        "try:\n",
        "    Xdf = pandas.DataFrame( X[:200,:] , columns=names )\n",
        "    Xdf_full = pandas.DataFrame( X , columns=names )\n",
        "\n",
        "    # Check the generation of the rample sample\n",
        "\n",
        "    import seaborn as sns\n",
        "    sns.pairplot( Xdf );\n",
        "    Xdf_full.corr()\n",
        "except:\n",
        "    print('Try to correct X value...')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_5UhE7SN-rQ"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "DPq2UGR4Em9N"
      },
      "source": [
        "## Question 5\n",
        "\n",
        "Estimate the number of samples required from the Monte Carlo simulation to reach 4 digits precision after the decimal point in the determination of $N_{\\nu}$. How do you proceed?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "Gn_w1SdrEm9Q"
      },
      "source": [
        "nsim_est = xxx\n",
        "print( 'Typical sample size to get a precision under 1e-4: ' , nsim_est )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHuaiLy5ODTx"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "mZA59wSmEm9U"
      },
      "source": [
        "Compute the mean and the standard deviation of the $N_{\\nu}$ sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "M12geVgREm9U"
      },
      "source": [
        "from matplotlib.pyplot import hist\n",
        "from numpy import mean, std, linspace\n",
        "from scipy.stats import norm\n",
        "\n",
        "\n",
        "nsim = xxx\n",
        "X = xxx\n",
        "Y = zeros(nsim)\n",
        "try:\n",
        "    for k in arange(nsim):\n",
        "        Y[k] = xxx\n",
        "\n",
        "    print('Toy MC estimate:')\n",
        "    print('Nnu = {:.4e}'.format(mean(Y)),'± {:1.4e}'.format(std(Y)))\n",
        "\n",
        "    # Compare this result with the Delta method output\n",
        "    print('Delta method estimate:')\n",
        "    print('Nnu = {:.4e}'.format(mu_Nnu),'± {:1.4e}'.format(sigma_Nnu))\n",
        "\n",
        "    # Draw a gaussian with the obtained mean and standard deviation\n",
        "    x = linspace( mean(Y)-4*std(Y) , mean(Y)+4*std(Y) , 100 )\n",
        "    hist( Y , 100 , density = True ); # histtype='step'\n",
        "    plot( x , norm.pdf( x , loc = mean(Y) , scale = std(Y) ));\n",
        "except:\n",
        "    print('===>>> Provide the expression for X and Y')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "eWQByLvWEm9X"
      },
      "source": [
        "# Check the normality with a qqplot\n",
        "import matplotlib.pyplot as plot\n",
        "from scipy.stats import probplot\n",
        "from matplotlib.pyplot import plot as plt\n",
        "probplot(Y, plot=plot);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhchXgWwOKta"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "7uNHcEK5Em9a"
      },
      "source": [
        "## Question 6\n",
        "\n",
        "Compute the correlation of $N_{\\nu}$ with the different random quantities involved in Table 1. Which variable is the most correlated to $N_{\\nu}$. The $N_{\\nu}$ formula does not depend on $\\Gamma_Z$. Which correlation value do you find between $\\Gamma_Z$ and $N_{\\nu}$. Why is this value not null despite the expression of $N_{\\nu}$ does not refer to $\\Gamma_Z$?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "4w0eS7HcEm9a"
      },
      "source": [
        "from numpy import corrcoef, concatenate, array\n",
        "YX = xxx\n",
        "try:\n",
        "    df(corrcoef(YX),columns=['Nnu',*names],index=['Nnu',*names])\n",
        "except:\n",
        "    print('Should try to build YX table where you could conccatenate X and Y = Nnu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ozum4FrGjJy"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "N0SA-8QAEm9e"
      },
      "source": [
        "This determination of $N_{\\nu}$ by LEP experiments proved that we should indeed expect the existence of 3 families of neutrinos despite only two were observed at that time. The tau neutrino was indeed discovered in year 2000 by the DONUT experiment (DONUT is an acronym which stands for Direct Observation of the NeUtrino Tau)."
      ]
    }
  ]
}